
# Philosophy of artificial intelligence

### From Wikipedia, the free encyclopedia

Jump to: navigation, search

<IMG> Artificial intelligence portal  
<IMG> Mind and Brain portal  
See also: ethics of artificial intelligence

The philosophy of artificial intelligence attempts to answer such question
as:[1]

  * Can a machine act intelligently? Can it solve any problem that a person would solve by thinking?
  * Can a machine have a mind, mental states and consciousness in the same sense humans do? Can it feel?
  * Are human intelligence and machine intelligence the same? Is the human brain essentially a computer?

These three questions reflect the divergent interests of AI researchers,
philosophers and cognitive scientists respectively. The answers to these
questions depend on how one defines "intelligence" or "consciousness" and
exactly which "machines" are under discussion.

Important propositions in the philosophy of AI include:

  * Turing's "polite convention": If a machine acts as intelligently as a human being, then it is as intelligent as a human being.[2]
  * The Dartmouth proposal: "Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it."[3]
  * Newell and Simon's physical symbol system hypothesis: "A physical symbol system has the necessary and sufficient means of general intelligent action."[4]
  * Searle's strong AI hypothesis: "The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds."[5]
  * Hobbes' mechanism: "Reason is nothing but reckoning."[6]

## Contents

  * 1 Can a machine display general intelligence?
    * 1.1 Intelligence
      * 1.1.1 Turing test
      * 1.1.2 Human intelligence vs. intelligence in general
    * 1.2 Arguments that a machine can display general intelligence
      * 1.2.1 The brain can be simulated
      * 1.2.2 Human thinking is symbol processing
      * 1.2.3 Arguments against symbol processing
        * 1.2.3.1 Lucas, Penrose and GÃ¶del
        * 1.2.3.2 Dreyfus: the primacy of unconscious skills
  * 2 Can a machine have a mind, consciousness and mental states?
    * 2.1 Consciousness, minds, mental states, meaning
    * 2.2 Arguments that a computer can't have a mind and mental states
      * 2.2.1 Searle's Chinese room
      * 2.2.2 Related arguments: Leibniz' mill, Block's telephone exchange and blockhead
      * 2.2.3 Responses to the Chinese Room
  * 3 Is thinking a kind of computation?
  * 4 Other related questions
    * 4.1 Can a machine have emotions?
    * 4.2 Can a machine be self aware?
    * 4.3 Can a machine be original or creative?
    * 4.4 Can a machine have a soul?
  * 5 See also
  * 6 Notes
  * 7 References
  * 8 External links

  
## [edit] Can a machine display general intelligence?

Is it possible to create a machine that can solve all the problems humans
solve using their intelligence? This is the question that AI researchers are
most interested in answering. It defines the scope of what machines will be
able to do in the future and guides the direction of AI research. It only
concerns the behavior of machines and ignores the issues of interest to
psychologists, cognitive scientists and philosophers; to answer this question,
it doesn't matter whether a machine is really thinking (as a person thinks) or
is just acting like it is thinking.[7]

The basic position of most AI researchers is summed up in this statement,
which appeared in the proposal for the Dartmouth Conferences of 1956:

  * Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it.[3]

Arguments against the basic premise must show that building a working AI
system is impossible, because there is some practical limit to the abilities
of computers or that there is some special quality of the human mind that is
necessary for thinking and yet can't be duplicated by a machine (or by the
methods of current AI research). Arguments in favor of the basic premise must
show that such a system is possible.

The first step to answering the question is to clearly define "intelligence."

### [edit] Intelligence

#### [edit] Turing test

Main article: Turing test

Alan Turing, in a famous and seminal 1950 paper,[8] reduced the problem of
defining intelligence to a simple question about conversation. He suggests
that: if a machine can answer any question put to it, using the same words
that an ordinary person would, then we may call that machine intelligent. A
modern version of his experimental design would use an online chat room, where
one of the participants is a real person and one of the participants is a
computer program. The program passes the test if no one can tell which of the
two participants is human.[2] Turing notes that no one (except philosophers)
ever asks the question "can people think?" He writes "instead of arguing
continually over this point, it is usual to have a polite convention that
everyone thinks."[9] Turing's test extends this polite convention to machines:

  * If a machine acts as intelligently as human being, then it is as intelligent as a human being.

#### [edit] Human intelligence vs. intelligence in general

One criticism of the Turing test is that it is explicitly anthropomorphic. If
our ultimate goal is to create machines that are more intelligent than people,
why should we insist that our machines must closely resemble people? Russell
and Norvig write that "aeronautical engineering texts do not define the goal
of their field as 'making machines that fly so exactly like pigeons that they
can fool other pigeons.'"[10] Recent AI research defines intelligence in terms
of intelligent agents. An "agent" is something which perceives and acts in an
environment. A "performance measure" defines what counts as success for the
agent.[11]

  * If an agent acts so as maximize the expected value of a performance measure based on past experience and knowledge then it is intelligent.[12]

Definitions like this one try to capture the essence of intelligence. They
have the advantage that, unlike the Turing test, they don't also test for
human traits that we may not want to consider intelligent, like the ability to
be insulted or the temptation to lie. They have the disadvantage that they
fail to make the commonsense differentiation between "things that think" and
"things that don't". By this definition, even a thermostat has a rudimentary
intelligence.

### [edit] Arguments that a machine can display general intelligence

#### [edit] The brain can be simulated

Main article: artificial brain

MRI.ogg

Play video

<IMG>

An MRI scan of a normal adult human brain

Marvin Minsky writes that "if the nervous system obeys the laws of physics and
chemistry, which we have every reason to suppose it does, then .... we ...
ought to be able to reproduce the behavior of the nervous system with some
physical device."[13] This argument, first introduced as early as 1943[14] and
vividly described by Hans Moravec in 1988,[15] is now associated with futurist
Ray Kurzweil, who estimates that computer power will be sufficient for a
complete brain simulation by the year 2029.[16]. A non-real-time simulation of
a thalamocortical model that has the size of the human brain (1011 neurons)
was performed in 2005 [1] and it took 50 days to simulate 1 second of brain
dynamics on a cluster of 27 processors (see also[2]).

Few disagree that a brain simulation is possible in theory, even critics of AI
such as Hubert Dreyfus and John Searle.[17] However, Searle points out that,
in principle, anything can be simulated by a computer, and so any process at
all can be considered "computation", if you're willing to stretch the
definition to the breaking point. "What we wanted to know is what
distinguishes the mind from thermostats and livers," he writes.[18] Any
argument that involves simply copying a brain is an argument that admits that
we know nothing about how intelligence works. "If we had to know how the brain
worked to do AI, we wouldn't bother with AI."[19]

#### [edit] Human thinking is symbol processing

Main article: physical symbol system

In 1963, Alan Newell and Herbert Simon proposed that "symbol manipulation" was
the essence of both human and machine intelligence. They wrote:

  * A physical symbol system has the necessary and sufficient means of general intelligent action.[4]

This claim is very strong: it implies both that human thinking is a kind of
symbol manipulation (because a symbol system is necessary for intelligence)
and that machines can be intelligent (because a symbol system is sufficient
for intelligence).[20] Another version of this position was described by
philosopher Hubert Dreyfus, who called it "the psychological assumption":

  * The mind can be viewed as a device operating on bits of information according to formal rules.[21]

A distinction is usually made between the kind of high level symbols that
directly correspond with objects in the world, such as <dog> and <tail> and
the more complex "symbols" that are present in a machine like a neural
network. Early research into AI, called "good old fashioned artificial
intelligence" (GOFAI) by John Haugeland, focused on these kind of high level
symbols.[22]

#### [edit] Arguments against symbol processing

These arguments show that human thinking does not consist (solely) of high
level symbol manipulation. They do not show that artificial intelligence is
impossible, only that more than symbol processing is required.

##### [edit] Lucas, Penrose and GÃ¶del

In 1931 Kurt GÃ¶del proved that it is always possible to create statements
that a formal system (such as an AI program) could not prove. A human being,
however, can (with some thought) see the truth of these "GÃ¶del statements".
This proved to philosopher John Lucas that human reason would always be
superior to machines.[23] He wrote "GÃ¶del's theorem seems to me to prove that
mechanism is false, that is, that minds cannot be explained as machines."[24]
Roger Penrose expanded on this argument in his 1989 book The Emperor's New
Mind, where he speculated that quantum mechanical processes inside individual
neurons gave humans this special advantage over machines.[25]

Douglas Hofstadter, in his Pulitzer prize winning book GÃ¶del, Escher, Bach:
An Eternal Golden Braid, explains that these "GÃ¶del-statements" always refer
to the system itself, similar to the way the Epimenides paradox uses
statements that refer to themselves, such as "this statement is false" or "I
am lying".[26] But, of course, the Epimenides paradox applies to anything that
makes statements, whether they are machines or humans, even Lucas himself.
Consider:

  * Lucas can't assert the truth of this statement.[27]

This statement is true but can't be asserted by Lucas. This shows that Lucas
himself is subject to the same limits that he describes for machines, as are
all people, and so Lucas's argument is pointless.[28]

Further, Russell and Norvig note that GÃ¶del's argument only applies to what
can theoretically be proved, given an infinite amount of memory and time. In
practice, real machines (including humans) have finite resources and will have
difficulty proving many theorems. It is not necessary to prove everything in
order to be intelligent.[29]

##### [edit] Dreyfus: the primacy of unconscious skills

Main article: Dreyfus' critique of artificial intelligence

Hubert Dreyfus argued that human intelligence and expertise depended primarily
on unconscious instincts rather than conscious symbolic manipulation, and
argued that these unconscious skills would never be captured in formal
rules.[30]

Dreyfus's argument had been anticipated by Turing in his 1950 paper Computing
machinery and intelligence, where he had classified this as the "argument from
the informality of behavior."[31] Turing argued in response that, just because
we don't know the rules that govern a complex behavior, this does not mean
that no such rules exist. He wrote: "we cannot so easily convince ourselves of
the absence of complete laws of behaviour ... The only way we know of for
finding such laws is scientific observation, and we certainly know of no
circumstances under which we could say, 'We have searched enough. There are no
such laws.'" [32]

Russell and Norvig point out that, in the years since Dreyfus published his
critique, progress has been made towards discovering the "rules" that govern
unconscious reasoning.[33] The situated movement in robotics research attempts
to capture our unconscious skills at perception and attention.[34]
Computational intelligence paradigms, such as neural nets, evolutionary
algorithms and so on are mostly directed at simulated unconscious reasoning
and learning. Research into commonsense knowledge has focused on reproducing
the "background" or context of knowledge. In fact, AI research in general has
moved away from high level symbol manipulation or "GOFAI", towards new models
that are intended to capture more of our unconscious reasoning. Historian and
AI researcher Daniel Crevier wrote that "time has proven the accuracy and
perceptiveness of some of Dreyfus's comments. Had he formulated them less
aggressively, constructive actions they suggested might have been taken much
earlier."[35]

## [edit] Can a machine have a mind, consciousness and mental states?

This is a philosophical question, related to the problem of other minds and
the hard problem of consciousness. The question revolves around a position
defined by John Searle as "strong AI":

  * A physical symbol system can have a mind and mental states.[5]

Searle distinguished this position from what he called "weak AI":

  * A physical symbol system can act intelligently.[5]

Searle introduced the terms to isolate strong AI from weak AI so he could
focus on what he thought was the more interesting and debatable issue. He
argued that even if we assume that we had a computer program that acted
exactly like a human mind, there would still be a difficult philosophical
question that needed to be answered.[5]

Neither of Searle's two positions are of great concern to AI research, since
they do not directly answer the question "can a machine display general
intelligence?" (unless it can also be shown that consciousness is necessary
for intelligence). There are a few researchers who believe that consciousness
is an essential element in intelligence, such as Igor Aleksander, Stan
Franklin, Ron Sun, and Pentti Haikonen, although their definition of
"consciousness" strays very close to "intelligence." See artificial
consciousness. Turing wrote "I do not wish to give the impression that I think
there is no mystery about consciousness ... [b]ut I do not think these
mysteries necessarily need to be solved before we can answer the question [of
whether machines can think]."[36] Russell and Norvig agree: "Most AI
researchers take the weak AI hypothesis for granted, and don't care about the
strong AI hypothesis."[37]

Before we can answer this question, we must be clear what we mean by "minds",
"mental states" and "consciousness".

### [edit] Consciousness, minds, mental states, meaning

<IMG>

<IMG>

Representation of consciousness from the 17th century.

The words "mind" and "consciousness" are used by different communities in
different ways. Some new age thinkers, for example, use the word
"consciousness" to describe something similar to Bergson's "Ã©lan vital": an
invisible, energetic fluid that permeates life and especially the mind.
Science fiction writers use the word to describe some essential property that
makes us human: a machine or alien that is "conscious" will be presented as a
fully human character, with intelligence, desires, will, insight, pride and so
on. (Science fiction writers also use the words "sentience", "sapience,"
"self-awareness" or "ghost" (as in the Ghost in the Shell manga and anime
series) to describe this essential human property.) For others, the words
"mind" or "consciousness" are used as a kind of secular synonym for the soul.

For philosophers, neuroscientists and cognitive scientists, the words are used
in a way that is both more precise and more mundane: they refer to the
familiar, everyday experience of having a "thought in your head", like a
perception, a dream, an intention or a plan, and to the way we know something,
or mean something or understand something. "It's not hard to give a
commonsense definition of consciousness" observes philosopher John Searle.[38]
What is mysterious and fascinating is not so much what it is but how it is:
how does a lump of fatty tissue and electricity give rise to this (familiar)
experience of perceiving, meaning or thinking?

Philosophers call this the hard problem of consciousness. It is the latest
version of a classic problem in the philosophy of mind called the "mind-body
problem."[39] A related problem is the problem of meaning or understanding
(which philosophers call "intentionality"): what is the connection between our
thoughts (i.e. patterns of neurons) and what we are thinking about (i.e.
objects and situations out in the world)? A third issue is the problem of
experience (or "phenomenology"): If two people see the same thing, do they
have the same experience? Or are there things "inside their head" (called
"qualia") that can be different from person to person?[40]

Neurobiologists believe all these problems will be solved as we begin to
identify the neural correlates of consciousness: the actual machinery in our
heads that creates the mind, experience and understanding. Even the harshest
critics of artificial intelligence agree that the brain is just a machine, and
that consciousness and intelligence are the result of physical processes in
the brain.[41] The difficult philosophical question is this: can a computer
program, running on a digital machine that shuffles the binary digits of zero
and one, duplicate the ability of the neurons to create minds, with mental
states (like understanding or perceiving), and ultimately, the experience of
consciousness?

### [edit] Arguments that a computer can't have a mind and mental states

#### [edit] Searle's Chinese room

Main article: Chinese room

John Searle asks us to consider a thought experiment: suppose we have written
a computer program that passes the Turing Test and demonstrates "general
intelligent action." Suppose, specifically that the program can converse in
fluent Chinese. Write the program on 3x5 cards and give them to an ordinary
person. Lock the person into a room and have him follow the instructions on
the cards. He will copy out Chinese characters and pass them in and out of the
room through a slot. From the outside, it will appear that the Chinese room
contains a fully intelligent person who speaks Chinese. The question is this:
is there anyone (or anything) in the room that understands Chinese? That is,
is there anything that has the mental state of understanding, or which has
conscious awareness of what is being discussed in Chinese? The man is clearly
not aware. The room can't be aware. The cards certainly aren't aware. Searle
concludes that the Chinese room, or any other physical symbol system, cannot
have a mind.[42]

Searle goes on to argue that actual mental states and consciousness require
(yet to be described) "actual physical-chemical properties of actual human
brains."[43] He argues there are special "causal properties" of brains and
neurons that gives rise to minds: in his words "brains cause minds."[44]

#### [edit] Related arguments: Leibniz' mill, Block's telephone exchange and
blockhead

Gottfried Leibniz made essentially the same argument as Searle in 1714, using
the thought experiment of expanding the brain until it was the size of a
mill.[45] In 1974, Lawrence Davis imagined duplicating the brain using
telephone lines and offices staffed by people, and in 1978 Ned Block
envisioned the entire population of China involved in such a brain simulation.
This thought experiment is called "the Chinese Nation" or "the Chinese
Gym".[46] Ned Block also proposed his "blockhead" argument, which is a version
of the Chinese room in which the program has been re-factored into a simple
set of rules of the form "see this, do that", removing all mystery from the
program.

#### [edit] Responses to the Chinese Room

Responses to the Chinese room emphasize several different points.

  1. The systems reply and the virtual mind reply:[47] This reply argues that the system, including the man, the program, the room, and the cards, is what understands Chinese. Searle claims that the man in the room is the only thing which could possibly "have a mind" or "understand", but others disagree, arguing that it is possible for there to be two minds in the same physical place, similar to the way a computer can simultaneously "be" two machines at once: one physical (like a Macintosh) and one "virtual" (like a word processor).
  2. Speed, power and complexity replies:[48] Several critics point out that the man in the room would probably take millions of years to respond to a simple question, and would require "filing cabinets" of astronomical proportions. This brings the clarity of Searle's intuition into doubt.
  3. Robot reply:[49] To truly understand, some believe the Chinese Room needs eyes and hands. Hans Moravec writes: 'If we could graft a robot to a reasoning program, we wouldn't need a person to provide the meaning anymore: it would come from the physical world."[50]
  4. Brain simulator reply:[51] What if the program simulates the sequence of nerve firings at the synapses of an actual brain of an actual Chinese speaker? The man in the room would be simulating an actual brain. This is a variation on the "systems reply" that appears more plausible because "the system" now clearly operates like a human brain, which strengthens the intuition that there is something besides the man in the room that could understand Chinese.
  5. Other minds reply and the epiphenomena reply:[52] Several people have noted that Searle's argument is just a version of the problem of other minds, applied to machines. Since it's difficult to decide if people are "actually" thinking, we shouldn't be surprised that it's difficult to answer the same question about machines. A related idea is that Searle's "causal properties" of neurons are epiphenomenal: they have no effect on the real world. Why would natural selection create them in the first place, if they make no difference to behavior?

## [edit] Is thinking a kind of computation?

Main article: computational theory of mind

This issue is of primary importance to cognitive scientists, who study the
nature of human thinking and problem solving.

The computational theory of mind or "computationalism" claims that the
relationship between mind and body is similar (if not identical) to the
relationship between a running program and a computer. The idea has
philosophical roots in Hobbes (who claimed reasoning was "nothing more than
reckoning"), Leibniz (who attempted to create a logical calculus of all human
ideas), Hume (who thought perception could be reduced to "atomic impressions")
and even Kant (who analyzed all experience as controlled by formal rules).[53]
The latest version is associated with philosophers Hilary Putnam and Jerry
Fodor.[54]

This question bears on our earlier questions: if the human brain is a kind of
computer then computers can be both intelligent and conscious, answering both
the practical and philosophical questions of AI. In terms of the practical
question of AI ("Can a machine display general intelligence?"), some versions
of computationalism make the claim that (as Hobbes wrote):

  * Reasoning is nothing but reckoning[6]

In other words, our intelligence derives from a form of calculation, similar
to arithmetic. This is the physical symbol system hypothesis discussed above,
and it implies that artificial intelligence is possible. In terms of the
philosophical question of AI ("Can a machine have mind, mental states and
consciousness?"), most versions of computationalism claim that (as Stevan
Harnad characterizes it):

  * Mental states are just implementations of (the right) computer programs[55]

This is John Searle's "strong AI" discussed above, and it is the real target
of the Chinese Room argument (according to Harnad).[55]

## [edit] Other related questions

Alan Turing noted that there are many arguments of the form "a machine will
never do X", where X can be many things, such as:

> Be kind, resourceful, beautiful, friendly, have initiative, have a sense of
humor, tell right from wrong, make mistakes, fall in love, enjoy strawberries
and cream, make someone fall in love with it, learn from experience, use words
properly, be the subject of its own thought, have as much diversity of
behaviour as a man, do something really new.[56]

Turing argues that these objections are often based on naive assumptions about
the versatility of machines or are "disguised forms of the argument from
consciousness". Writing a program that exhibits one of these behaviors "will
not make much of an impression."[56] All of these arguments are tangential to
the basic premise of AI, unless it can be shown that one of these traits is
essential for general intelligence.

### [edit] Can a machine have emotions?

Hans Moravec believes that "robots in general will be quite emotional about
being nice people"[57] and describes emotions in terms of the behaviors they
cause. Fear is a source of urgency. Empathy is a necessary component of good
human computer interaction. He says robots "will try to please you in an
apparently selfless manner because it will get a thrill out of this positive
reinforcement. You can interpret this as a kind of love."[57] Daniel Crevier
writes "Moravec's point is that emotions are just devices for channeling
behavior in a direction beneficial to the survival of one's species."[58]

The question of whether the machine actually feels an emotion, or whether it
merely acts as if feeling an emotion is the philosophical question, "can a
machine be conscious?" in another form.[36]

### [edit] Can a machine be self aware?

"Self awareness", as noted above, is sometimes used by science fiction writers
as a name for the essential human property that makes a character fully human.
Turing strips away all other properties of human beings and reduces the
question to "can a machine be the subject of its own thought?" Can it think
about itself? Viewed in this way, it is obvious that a program can be written
that can report on its own internal states, such as a debugger.[56]

### [edit] Can a machine be original or creative?

Turing reduces this to the question of whether a machine can âtake us by
surprise" and argues that this is obviously true, as any programmer can
attest.[59] He notes that, with enough storage capacity, a computer can behave
in an astronomical number of different ways.[60] It must be possible, even
trivial, for a computer that can represent ideas to combine them in new ways.
(Douglas Lenat's Automated Mathematician, as one example, combined ideas to
discover new mathematical truths.)

In 2009, scientists at Aberystwyth University in Wales and the U.K's
University of Cambridge designed a robot called Adam that they believe to be
the first machine to independently come up with new scientific findings.[61].
Also in 2009, researchers at Cornell developed a computer program that
extrapolated the laws of motion from a pendulum's

### [edit] Can a machine have a soul?

Finally, those who believe in the existence of a soul would argue that

  * Thinking is a function of manâs immortal soul

Alan Turing called this âthe theological objectionâ and writes

> In attempting to construct such machines we should not be irreverently
usurping His power of creating souls, any more than we are in the procreation
of children: rather we are, in either case, instruments of His will providing
mansions for the souls that He creates.[62]

## [edit] See also

  * Artificial intelligence
  * Philosophy of information
  * Philosophy of mind
  * Brain (other matters section)
  * Computational theory of mind
  * Functionalism

  * Turing Test
  * Artificial brain
  * Physical symbol system
  * Dreyfus' critique of artificial intelligence
  * Chinese room
  * Computing Machinery and Intelligence

  
## [edit] Notes

  1. ^ Russell & Norvig 2003, p. 947 define the philosophy of AI as consisting of the first two questions, and the additional question of the ethics of artificial intelligence. Fearn 2007, p. 55 writes "In the current literature, philosophy has to chief roles: to determine whether or not such machines would be conscious, and, second, to predict whether or not such machines are possible." The last question bears on the first two.
  2. ^ a b This is a paraphrase of the essential point of the Turing Test. Turing 1950, Haugeland 1985, pp. 6-9, Crevier 1993, p. 24, Russell & Norvig 2003, pp. 2-3 and 948
  3. ^ a b McCarthy et al. 1955. This assertion was printed in the program for the Dartmouth Conference of 1956, widely considered the "birth of AI."also Crevier 1993, p. 28
  4. ^ a b Newell & Simon 1976 and Russell & Norvig 2003, p. 18
  5. ^ a b c d This version is from Searle (1999), and is also quoted in Dennett 1991, p. 435\. Searle's original formulation was "The appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states." (Searle 1980, p. 1). Strong AI is defined similarly by Russell & Norvig (2003, p. 947): "The assertion that machines could possibly act intelligently (or, perhaps better, act as if they were intelligent) is called the 'weak AI' hypothesis by philosophers, and the assertion that machines that do so are actually thinking (as opposed to simulating thinking) is called the 'strong AI' hypothesis."
  6. ^ a b Hobbes 1651, chpt. 5
  7. ^ See Russell & Norvig 2003, p. 3, where they make the distinction between acting rationally and being rational, and define AI as the study of the former.
  8. ^ Turing 1950 and see Russell & Norvig 2003, p. 948, where they call his paper "famous" and write "Turing examined a wide variety of possible objections to the possibility of intelligent machines, including virtually all of those that have been raised in the half century since his paper appeared."
  9. ^ Turing 1950 under "The Argument from Consciousness"
  10. ^ Russell & Norvig 2003, p. 3
  11. ^ Russell & Norvig 2003, p. 4-5, 32, 35, 36 and 56
  12. ^ Russell and Norvig would prefer the word "rational" to "intelligent".
  13. ^ Crevier 1993, p. 125
  14. ^ Pitts & McCullough 1943
  15. ^ Moravec 1988
  16. ^ Kurzweil 2005, p. 262\. Also see Russell Norvig, p. 957 and Crevier 1993, pp. 271 and 279. The most extreme form of this argument (the brain replacement scenario) was put forward by Clark Glymour in the mid-70s and was touched on by Zenon Pylyshyn and John Searle in 1980
  17. ^ Hubert Dreyfus writes: "In general, by accepting the fundamental assumptions that the nervous system is part of the physical world and that all physical processes can described in a mathematical formalism which can in turn be manipulated by a digital computer, one can arrive at the strong claim that the behavior which results from human 'information processing,' whether directly formalizable or not, can always be indirectly reproduced on a digital machine." (Dreyfus 1972, pp. 194-5). John Searle writes: "Could a man made machine think? Assuming it possible produce artificially a machine with a nervous system, ... the answer to the question seems to be obviously, yes ... Could a digital computer think? If by 'digital computer' you mean anything at all that has a level of description where it can be correctly described as the instantiation of a computer program, then again the answer is, of course, yes, since we are the instantiations of any number of computer programs, and we can think." (Searle 1980, p. 11)
  18. ^ Searle 1980, p. 7
  19. ^ Searle 1980, p. 14
  20. ^ Searle writes "I like the straight forwardness of the claim." Searle 1980, p. 4
  21. ^ Dreyfus 1979, p. 156
  22. ^ Haugeland 1985, p. 5
  23. ^ Lucas 1961, Russell & Norvig 2003, pp. 949-950, Hofstadter 1979, pp. 471-473,476-477, Turing 1950 under âThe Argument from Mathematicsâ
  24. ^ Lucas 1961, p. 57-9
  25. ^ Penrose 1989
  26. ^ Hofstadter 1979
  27. ^ According to Hofstadter 1979, p. 476-477, this statement was first proposed by C. H. Whitely
  28. ^ Hofstadter 1979, pp. 476-477, Russell & Norvig 2003, p. 950, Turing 1950 under âThe Argument from Mathematicsâ where he writes âalthough it is established that there are limitations to the powers of any particular machine, it has only been stated, without sort of proof, that no such limitations apply to the human intellect.â
  29. ^ Russell & Norvig 2003, p. 950\. They point out that real machines with finite memory can be modeled using first order logic, which is formally decidable, and GÃ¶del's argument does not apply to them at all.
  30. ^ Dreyfus 1972, Dreyfus 1979, Dreyfus & Dreyfus 1986. See also Russell & Norvig 2003, pp. 950-952, Crevier & 1993 120-132 and Hearn 2007, pp. 50-51
  31. ^ Russell & Norvig 2003, p. 950-51
  32. ^ Turing 1950 under "(8) The Argument from the Informality of Behavior"
  33. ^ Russell & Norvig 2003, p. 52
  34. ^ See Brooks 1990 and Moravec 1988
  35. ^ Crevier 1993, p. 125
  36. ^ a b Turing 1950 under â(4) The Argument from Consciousnessâ. See also Russell Norvig, p. 952-3, where they identify Searle's argument with Turing's "Argument from Consciousness."
  37. ^ Russell & Norvig 2003, p. 947
  38. ^ "[P]eople always tell me it was very hard to define consciousness, but I think if you're just looking for the kind of commonsense definition that you get at the beginning of the investigation, and not at the hard nosed scientific definition that comes at the end, it's not hard to give commonsense definition of consciousness." The Philosopher's Zone: The question of consciousness. Also see Dennett 1991
  39. ^ Blackmore 2005, p. 2
  40. ^ Russell & Norvig 2003, p. 954-956
  41. ^ For example, John Searle writes: "Can a machine think? The answer is, obvious, yes. We are precisely such machines." (Searle 1980, p. 11)
  42. ^ Searle 1980. See also Cole 2004, Russell & Norvig 2003, pp. 958-960, Crevier 1993, pp. 269-272 and Hearn 2007, pp. 43-50
  43. ^ Searle 1980, p. 13
  44. ^ Searle 1984
  45. ^ Cole 2004, 2.1, Leibniz 1714, 17
  46. ^ Cole 2004, 2.3
  47. ^ Searle 1980 under "1. THe Systems Reply (Berkeley)", Crevier 1993, p. 269, Russell & Norvig 2003, p. 959, Cole 2004, 4.1. Among those who hold to the "system" position (according to Cole) are Ned Block, Jack Copeland, Daniel Dennett, Jerry Fodor, John Haugeland, Ray Kurzweil and Georges Rey. Those who have defended the "virtual mind" reply include Marvin Minsky, Alan Perlis, David Chalmers, Ned Block and J. Cole (again, according to Cole 2004)
  48. ^ Cole 2004, 4.2 ascribes this position to Ned Block, Daniel Dennett, Tim Maudlin, David Chalmers, Steven Pinker, Patricia Churchland and others.
  49. ^ Searle 1980 under "2. The Robot Reply (Yale)". Cole 2004, 4.3 ascribes this position to Margaret Boden, Tim Crane, Daniel Dennett, Jerry Fodor, Stevan Harnad, Hans Moravec and Georges Rey
  50. ^ Quoted in Crevier 1993, p. 272
  51. ^ Searle 1980 under "3. The Brain Simulator Reply (Berkeley and M.I.T.)" Cole 2004 ascribes this position to Paul and Patricia Churchland and Ray Kurzweil
  52. ^ Searle 1980 under "5. The Other Minds Reply", Cole 2004, 4.4. Turing 1950 makes this reply under "(4) The Argument from Consciousness." Cole ascribes this position to Daniel Dennett and Hans Moravec.
  53. ^ Dreyfus 1979, p. 156, Haugeland, pp. 15-44
  54. ^ Horst 2005
  55. ^ a b Harnad 2001
  56. ^ a b c Turing 1950 under "(5) Arguments from Various Disabilities"
  57. ^ a b Quoted in Crevier 1993, p. 266
  58. ^ Crevier 1993, p. 266
  59. ^ Turing 1950 under "(6) Lady Lovelace's Objection"
  60. ^ Turing 1950 under "(5) Argument from Various Disabilities"
  61. ^ http://news.cnet.com/8301-17938_105-10211175-1.html?tag=newsLatestHeadlinesArea.0
  62. ^ Turing 1950 under "(1) The Theological Objectionâ, although it should be noted that he also writes âI am not very impressed with theological arguments whatever they may be used to supportâ

## [edit] References

  * Blackmore, Susan (2005), Consciousness: A Very Short Introduction, Oxford University Press
  * Brooks, Rodney (1990), "Elephants Don't Play Chess" (PDF), Robotics and Autonomous Systems 6: 3â15, doi:10.1016/S0921-8890(05)80025-9, http://people.csail.mit.edu/brooks/papers/elephants.pdf, retrieved on 2007-08-30
  * Cole, David (Fall 2004), "The Chinese Room Argument", in Zalta, Edward N., The Stanford Encyclopedia of Philosophy, http://plato.stanford.edu/archives/fall2004/entries/chinese-room/ .
  * Crevier, Daniel (1993), AI: The Tumultuous Search for Artificial Intelligence, New York, NY: BasicBooks, ISBN 0-465-02997-3
  * Dennett, Daniel (1991), Consciousness Explained, The Penguin Press, ISBN 0-7139-9037-6
  * Dreyfus, Hubert (1972), What Computers Can't Do, New York: MIT Press, ISBN 0060110821
  * Dreyfus, Hubert (1979), What Computers Still Can't Do, New York: MIT Press .
  * Dreyfus, Hubert; Dreyfus, Stuart (1986), Mind over Machine: The Power of Human Intuition and Expertise in the Era of the Computer, Oxford, UK: Blackwell
  * Fearn, Nicholas (2007), The Latest Answers to the Oldest Questions: A Philosophical Adventure with the World's Greatest Thinkers, New York: Grove Press
  * Gladwell, Malcolm (2005), Blink: The Power of Thinking Without Thinking, Boston: Little, Brown, ISBN 0-316-17232-4 .
  * Harnad, Stevan (2001), "What's Wrong and Right About Searle's Chinese Room Argument?", in Bishop, M.; Preston, J., Essays on Searle's Chinese Room Argument, Oxford University Press, http://cogprints.org/4023/1/searlbook.htm
  * Hobbes (1651), Leviathan .
  * Hofstadter, Douglas (1979), GÃ¶del, Escher, Bach: an Eternal Golden Braid .
  * Horst, Steven (Fall 2005), "The Computational Theory of Mind", in Zalta, Edward N., The Stanford Encyclopedia of Philosophy, http://plato.stanford.edu/archives/fall2005/entries/computational-mind/ .
  * Kurzweil, Ray (2005), The Singularity is Near, New York: Viking Press, ISBN 0-670-03384-7 .
  * Lucas, John (1961), "Minds, Machines and GÃ¶del", in Anderson, A.R., Minds and Machines, http://users.ox.ac.uk/~jrlucas/Godel/mmg.html .
  * McCarthy, John; Minsky, Marvin; Rochester, Nathan; Shannon, Claude (1955), A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html .
  * McDermott, Drew (May 14, 1997), "How Intelligent is Deep Blue", New York Times, http://www.psych.utoronto.ca/~reingold/courses/ai/cache/mcdermott.html
  * Moravec, Hans (1988), Mind Children, Harvard University Press
  * Newell, Allen; Simon, H. A. (1963), "GPS: A Program that Simulates Human Thought", in Feigenbaum, E.A.; Feldman, J., Computers and Thought, McGraw-Hill
  * Newell, Allen; Simon, H. A. (1976), "Computer Science as Empirical Inquiry: Symbols and Search", Communications of the ACM, 19, http://www.rci.rutgers.edu/~cfs/472_html/AI_SEARCH/PSS/PSSH4.html
  * Russell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, NJ: Prentice Hall, ISBN 0-13-790395-2, http://aima.cs.berkeley.edu/
  * Penrose, Roger (1989), The Emperor's New Mind: Concerning Computers, Minds, and The Laws of Physics, Oxford University Press, ISBN 0-14-014534-6
  * Searle, John (1980), "Minds, Brains and Programs", Behavioral and Brain Sciences 3 (3): 417â457, http://members.aol.com/NeoNoetics/MindsBrainsPrograms.html
  * Searle, John (1992), The Rediscovery of the the Mind, Cambridge, Massachusetts: M.I.T. Press
  * Searle, John (1999), Mind, language and society, New York, NY: Basic Books, ISBN 0465045219, OCLC 231867665 43689264
  * Turing, Alan (October 1950), "Computing Machinery and Intelligence", Mind LIX (236): 433â460, doi:10.1093/mind/LIX.236.433, ISSN 0026-4423, http://loebner.net/Prizef/TuringArticle.html, retrieved on 2008-08-18

## [edit] External links

  * Research Paper: Philosophy of Consciousness and Ethics In Artificial Intelligence

Retrieved from
"http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence"

Categories: Philosophy of artificial intelligence | Philosophy by field

##### Views

  * Article
  * Discussion
  * Edit this page
  * History

##### Personal tools

  * Log in / create account

##### Navigation

  * Main page
  * Contents
  * Featured content
  * Current events
  * Random article

##### Search



##### Interaction

  * About Wikipedia
  * Community portal
  * Recent changes
  * Contact Wikipedia
  * Donate to Wikipedia
  * Help

##### Toolbox

  * What links here
  * Related changes
  * Upload file
  * Special pages
  * Printable version
  * Permanent link
  * Cite this page

##### Languages

  * ÙØ§Ø±Ø³Û
  * Ð ÑÑÑÐºÐ¸Ð¹

Powered by MediaWiki

Wikimedia Foundation

  * This page was last modified on 22 April 2009, at 22:41 (UTC).
  * All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)   
Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S.
registered 501(c)(3) tax-deductible nonprofit charity.  

  * Privacy policy
  * About Wikipedia
  * Disclaimers



