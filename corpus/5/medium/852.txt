
# Self-organizing map

### From Wikipedia, the free encyclopedia

Jump to: navigation, search

A self-organizing map (SOM) is a type of artificial neural network that is
trained using unsupervised learning to produce a low-dimensional (typically
two dimensional), discretized representation of the input space of the
training samples, called a map. Self-organizing maps are different than other
artificial neural networks in the sense that they use a neighborhood function
to preserve the topological properties of the input space.

<IMG>

<IMG>

A self-organizing map showing US Congress voting patterns visualized in
Synapse. The first two boxes show clustering and distances while the remaining
ones show the component planes. Red means a yes vote while blue means a no
vote in the component planes (except the party component where red is
Republican and blue is Democrat).

This makes SOM useful for visualizing low-dimensional views of high-
dimensional data, akin to multidimensional scaling. The model was first
described as an artificial neural network by the Finnish professor Teuvo
Kohonen, and is sometimes called a Kohonen map. [1]

Like most artificial neural networks, SOMs operate in two modes: training and
mapping. Training builds the map using input examples. It is a competitive
process, also called vector quantization. Mapping automatically classifies a
new input vector.

## Contents

  * 1 Network structure
  * 2 Learning algorithm
  * 3 Example
    * 3.1 Preliminary definitions
    * 3.2 Variables
    * 3.3 Stepping through the algorithm
  * 4 Interpretation
  * 5 Alternatives
  * 6 References
  * 7 Further reading
  * 8 See also
  * 9 External links

  
## [edit] Network structure

A self-organizing map consists of components called nodes or neurons.
Associated with each node is a weight vector of the same dimension as the
input data vectors and a position in the map space. The usual arrangement of
nodes is a regular spacing in a hexagonal or rectangular grid. The self-
organizing map describes a mapping from a higher dimensional input space to a
lower dimensional map space. The procedure for placing a vector from data
space onto the map is to find the node with the closest weight vector to the
vector taken from data space and to assign the map coordinates of this node to
our vector.

While it is typical to consider this type of network structure as related to
feedforward networks where the nodes are visualized as being attached, this
type of architecture is fundamentally different in arrangement and motivation.

Useful extensions include using toroidal grids where opposite edges are
connected and using large numbers of nodes. It has been shown that while self-
organizing maps with a small number of nodes behave in a way that is similar
to K-means, larger self-organizing maps rearrange data in a way that is
fundamentally topological in character.

It is also common to use the U-matrix. The U-matrix value of a particular node
is the average distance between the node and its closest neighbors. In a
rectangular grid for instance, we might consider the closest 4 or 8 nodes.

Large SOMs display properties which are emergent. Therefore, large maps are
preferable to smaller ones. In maps consisting of thousands of nodes, it is
possible to perform cluster operations on the map itself.[2]

## [edit] Learning algorithm

The goal of learning in the self-organizing map is to cause different parts of
the network to respond similarly to certain input patterns. This is partly
motivated by how visual, auditory or other sensory information is handled in
separate parts of the cerebral cortex in the human brain.[3]

The weights of the neurons are initialized either to small random values or
sampled evenly from the subspace spanned by the two largest principal
component eigenvectors. With the latter alternative, learning is much faster
because the initial weights already give good approximation of SOM weights.[4]

The network must be fed a large number of example vectors that represent, as
close as possible, the kinds of vectors expected during mapping. The examples
are usually administered several times.

The training utilizes competitive learning. When a training example is fed to
the network, its Euclidean distance to all weight vectors is computed. The
neuron with weight vector most similar to the input is called the best
matching unit (BMU). The weights of the BMU and neurons close to it in the SOM
lattice are adjusted towards the input vector. The magnitude of the change
decreases with time and with distance from the BMU. The update formula for a
neuron with weight vector Wv(t) is

    Wv(t + 1) = Wv(t) + Î (v, t) Î±(t)(D(t) - Wv(t)),
where Î±(t) is a monotonically decreasing learning coefficient and D(t) is the
input vector. The neighborhood function Î (v, t) depends on the lattice
distance between the BMU and neuron v. In the simplest form it is one for all
neurons close enough to BMU and zero for others, but a gaussian function is a
common choice, too. Regardless of the functional form, the neighborhood
function shrinks with time.[3] At the beginning when the neighborhood is
broad, the self-organizing takes place on the global scale. When the
neighborhood has shrunk to just a couple of neurons the weights are converging
to local estimates.

This process is repeated for each input vector for a (usually large) number of
cycles Î». The network winds up associating output nodes with groups or
patterns in the input data set. If these patterns can be named, the names can
be attached to the associated nodes in the trained net.

During mapping, there will be one single winning neuron: the neuron whose
weight vector lies closest to the input vector. This can be simply determined
by calculating the Euclidean distance between input vector and weight vector.

While representing input data as vectors has been emphasized in this article,
it should be noted that any kind of object which can be represented digitally
and which has an appropriate distance measure associated with it and in which
the necessary operations for training are possible can be used to construct a
self-organizing map. This includes matrices, continuous functions or even
other self-organizing maps.

## [edit] Example

### [edit] Preliminary definitions

Consider a 10Ã10 array of nodes each of which contains a weight vector and is
aware of its location in the array. Each weight vector is of the same
dimension as the node's input vector. The weights are initially set to random
values.

Now we need input to feed the map. (The generated map and the given input
exist in separate subspaces.) We will create three vectors to represent
colors. Colors can be represented by their red, green, and blue components.
Consequently our input vectors will have three components, each corresponding
to a color space. The input vectors will be:

    
    R = <255, 0, 0>
    G = <0, 255, 0>
    B = <0, 0, 255>
    
### [edit] Variables

Vectors are in bold

    
    t = current iteration
    Î» = limit on time iteration
    Wv = current weight vector
    D = target input
    Î(t) = restraint due to distance from BMU - usually called the neighbourhood function
    Î±(t) = learning restraint due to time
    
### [edit] Stepping through the algorithm

  1. Randomize the map's nodes' weight vectors
  2. Grab an input vector
  3. Traverse each node in the map 
    1. Use Euclidean distance formula to find similarity between the input vector and the map's node's weight vector
    2. Track the node that produces the smallest distance (this node is the best matching unit, BMU)
  4. Update the nodes in the neighbourhood of BMU by pulling them closer to the input vector 
    1. Wv(t + 1) = Wv(t) + Î(t)Î±(t)(D(t) - Wv(t))
  5. Increment t and repeat from 2 while t < Î»

## [edit] Interpretation

There are two ways to interpret a SOM. Because in the training phase weights
of the whole neighborhood are moved in the same direction, similar items tend
to excite adjacent neurons. Therefore, SOM forms a semantic map where similar
samples are mapped close together and dissimilar apart.

The other way is to think of neuronal weights as pointers to the input space.
They form a discrete approximation of the distribution of training samples.
More neurons point to regions with high training sample concentration and
fewer where the samples are scarce.

SOM may be considered a nonlinear generalization of Principal components
analysis (PCA) [5]. It has been shown, using both artificial and real
geophysical data, that SOM has many advantages [6] [7] over the conventional
feature extraction methods such as Empirical Orthogonal Functions (EOF) or
PCA.

## [edit] Alternatives

Generative topographic maps (GTM) are a potential alternative to SOMs. In the
sense that GTM explicitly requires a smooth and continuous mapping from the
input space to the map space, it is topology preserving. However, in a
practical sense, this measure of topological preservation is lacking.[8]

## [edit] References

  1. ^ Kohonen, T. and Honkela, T. (2007). "Kohonen network". Scholarpedia. http://www.scholarpedia.org/article/Kohonen_network.
  2. ^ Ultsch, Alfred (2007). Emergence in Self-Organizing Feature Maps, In Proceedings Workshop on Self-Organizing Maps (WSOM '07). Bielefeld, Germany. ISBN 978-3-00-022473-7.
  3. ^ a b Haykin, Simon (1999). "9. Self-organizing maps". Neural networks - A comprehensive foundation (2nd edition ed.). Prentice-Hall. ISBN 0-13-908385-5.
  4. ^ "Intro to SOM by Teuvo Kohonen". SOM Toolbox. http://www.cis.hut.fi/projects/somtoolbox/theory/somalgorithm.shtml. Retrieved on 2006-06-18.
  5. ^ Yin H. Learning Nonlinear Principal Manifolds by Self-Organising Maps, In: Gorban A. N. et al (Eds.), LNCSE 58, Springer, 2007 ISBN 978-3-540-73749-0
  6. ^ Liu, Y., and R.H. Weisberg (2005), Patterns of ocean current variability on the West Florida Shelf using the self-organizing map. Journal of Geophysical Research, 110, C06003, doi:10.1029/2004JC002786.
  7. ^ Liu, Y., R.H. Weisberg, and C.N.K. Mooers (2006), Performance evaluation of the Self-Organizing Map for feature extraction. Journal of Geophysical Research, 111, C05018, doi:10.1029/2005jc003117.
  8. ^ Kaski, S.. Data exploration using self-organizing maps, Acta Polytechnica Scandinavica, Mathematics, Computing and Management in Engineering Series No. 82, Espoo 1997, 57 pp..

## [edit] Further reading

Performance Evaluation of the Self-Organizing Map in feature extraction using
time series of known patterns.

Rustum R., Adeloye A.J., and Scholz M. , 2008. Applying Kohonen Self-
organizing Map as a Software Sensor to Predict the Biochemical Oxygen Demand.
Water Environment Research, 80 (1), 2008.

Rustum R and A.J.Adeloye, 2007. Replacing outliers and missing values from
activated sludge data using Kohonen Self Organizing Map. Journal of
Environmental Engineering, Vol. 133,No. 9, September 1, 2007, pp. 909-916.

## [edit] See also

  * Artificial intelligence
  * Biologically-inspired computing
  * Connectionism
  * Data mining
  * Growing self-organizing map
  * Growing neural gas
  * Generative topographic map
  * Nonlinear dimensionality reduction
  * Machine learning
  * Pattern recognition
  * Predictive analytics
  * Radial basis function network
  * Artificial Neural Network
  * Sammon's projection
  * J. Brant Arseneau

## [edit] External links

  * Chapter 15 Kohonen networks of Neural Networks - A Systematic Introduction by RaÃºl Rojas (ISBN 978-3540605058)
  * Prof. Kohonen's website in Helsinki University of Technology See ->research ->software for Toolkits and C and Matlab code for SOM's
  * Kohonen network a Scholarpedia article on the Self-Organizing Map
  * "The Self-Organized Gene, Part 1", and Part 2 beginners' level introduction to competitive learning and self-organizing maps.
  * Chapter 7: Competition and self organisation: Kohonen nets, part of Kevin Gurney's web-book on Neural Nets.
  * NeuronDotNet, Implementation in C# along with sample applications
  * Nice application to visualize some neural-network algorithms. Written in Java, so you need a Java-plugin in your browser.
  * Programming a Kohonen Neural Network in Java part of a Java Neural Network web book.
  * Databionics Prof. A. Ultsch's websites on Visualization and Datamining with SOM
  * Kohonen Neural Network applied to the Traveling Salesman Problem (using three dimensions).
  * Python SOM example: Simple SOM python library along with a 2D implementation and some very suggestive images.
  * Viscovery SOMine: SOM Technology Tool from Viscovery Software (formerly Eudaptics Software)
  * SomColour tutorial: Self-Organising Maps For Colour Recognition
  * WEBSOM, a Kohonen network project
  * Staff-Mapper, Solution for mapping knowledge based on survey results
  * Kohonen Neural Network Simulation, Demonstration of and detailed discussion about Kohonen Neural Networks, a form of SOM
  * The Semantic Map Interface based on Kohonen Map for Text analysis and visualization on netzspannung.org\- the German platform for media art & electronic culture.
  * Kohonen Map Java Applet
  * SOM and Sammon's mapping software for multivariate data analysis

Retrieved from "http://en.wikipedia.org/wiki/Self-organizing_map"

Categories: Machine learning | Neural networks

##### Views

  * Article
  * Discussion
  * Edit this page
  * History

##### Personal tools

  * Log in / create account

##### Navigation

  * Main page
  * Contents
  * Featured content
  * Current events
  * Random article

##### Search



##### Interaction

  * About Wikipedia
  * Community portal
  * Recent changes
  * Contact Wikipedia
  * Donate to Wikipedia
  * Help

##### Toolbox

  * What links here
  * Related changes
  * Upload file
  * Special pages
  * Printable version
  * Permanent link
  * Cite this page

##### Languages

  * Ø§ÙØ¹Ø±Ø¨ÙØ©
  * Deutsch
  * EspaÃ±ol
  * FranÃ§ais
  * íêµ­ì´
  * Italiano
  * Nederlands
  * æ¥æ¬èª
  * Polski
  * Ð ÑÑÑÐºÐ¸Ð¹
  * SlovenÅ¡Äina
  * Suomi
  * Ð£ÐºÑÐ°ÑÐ½ÑÑÐºÐ°

Powered by MediaWiki

Wikimedia Foundation

  * This page was last modified on 2 April 2009, at 14:34.
  * All text is available under the terms of the GNU Free Documentation License. (See Copyrights for details.)   
Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a U.S.
registered 501(c)(3) tax-deductible nonprofit charity.  

  * Privacy policy
  * About Wikipedia
  * Disclaimers



